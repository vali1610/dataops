name: Deploy and Run Spark Job on Dataproc

on:
  workflow_dispatch:
    inputs:
      project_id:
        description: "GCP Project ID"
        required: true
        default: "dataops-466411"
      region:
        description: "GCP Region"
        required: true
        default: "europe-west1"
      bucket_name:
        description: "GCS Bucket Name"
        required: true
        default: "vale-dataops-bucket"
      dataproc_cluster:
        description: "Dataproc Cluster Name"
        required: true
        default: "vale-dataproc"
      customer_csv:
        description: "Path to customer CSV in GCS"
        required: true
        default: "gs://vale-dataops-bucket/data/customer.csv"
      payment_csv:
        description: "Path to payment CSV in GCS"
        required: true
        default: "gs://vale-dataops-bucket/data/payment.csv"

jobs:
  deploy_and_run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Auth GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1

      - name: Upload Spark job and input CSVs to GCS
        run: |
          gsutil cp ./local_processing/pyspark_job.py gs://vale-dataops-bucket/jobs/
          gsutil cp ./customer_data_dirty.csv gs://vale-dataops-bucket/data/
          gsutil cp ./payment_data_dirty.csv gs://vale-dataops-bucket/data/

      - name: Submit Spark job to Dataproc
        run: |
          gcloud dataproc jobs submit pyspark gs://vale-dataops-bucket/jobs/spark_etl_job.py \
            --cluster=vale-dataproc \
            --region=europe-west1 \
            --project=dataops-466411 \
            -- \
            gs://vale-dataops-bucket/data/customer_data_dirty.csv \
            gs://vale-dataops-bucket/data/payment_data_dirty.csv \
            --jars=gs://vale-dataops-bucket/jars/delta-core_2.12-2.1.0.jar \
            --properties="spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension,spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"

