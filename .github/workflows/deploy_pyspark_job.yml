name: Deploy and Run Spark Job on Dataproc

on:
  workflow_dispatch:
    inputs:
      project_id:
        description: "GCP Project ID"
        required: true
        default: "dataops-466411"
      region:
        description: "GCP Region"
        required: true
        default: "europe-west1"
      bucket_name:
        description: "GCS Bucket Name"
        required: true
        default: "vale-dataops-bucket"
      dataproc_cluster:
        description: "Dataproc Cluster Name"
        required: true
        default: "vale-dataproc"
      customer_csv:
        description: "Path to customer CSV in GCS"
        required: true
        default: "gs://vale-dataops-bucket/data/customer.csv"
      payment_csv:
        description: "Path to payment CSV in GCS"
        required: true
        default: "gs://vale-dataops-bucket/data/payment.csv"

jobs:
  deploy_and_run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Auth GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1

      - name: Upload Spark job and input CSVs to GCS
        run: |
          gsutil cp ./local_processing/pyspark_job.py gs://vale-dataops-bucket/jobs/
          gsutil cp ./customer_data_dirty.csv gs://vale-dataops-bucket/data/
          gsutil cp ./payment_data_dirty.csv gs://vale-dataops-bucket/data/

      - name: Download and Upload JAR dependencies to GCS
        run: |
          mkdir -p deps

          wget -O deps/delta-spark_2.12-3.2.0.jar https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.0/delta-spark_2.12-3.2.0.jar
          

          wget -O deps/iceberg-spark-runtime-3.5_2.12-1.5.0.jar https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.0/iceberg-spark-runtime-3.5_2.12-1.5.0.jar
          

          wget -O deps/hudi-spark3.5-bundle_2.12-1.0.2.jar https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3.5-bundle_2.12/1.0.2/hudi-spark3.5-bundle_2.12-1.0.2.jar
          

          gsutil cp deps/* gs://vale-dataops-bucket/libs/



      - name: Submit Spark job to Dataproc
        run: |
          gcloud dataproc jobs submit pyspark gs://vale-dataops-bucket/jobs/pyspark_job.py \
            --cluster=vale-dataproc \
            --region=europe-west1 \
            --jars=gs://vale-dataops-bucket/libs/delta-spark_2.12-3.2.0.jar,gs://vale-dataops-bucket/libs/hudi-spark3.5-bundle_2.12-1.0.2.jar,gs://vale-dataops-bucket/libs/iceberg-spark-runtime-3.5_2.12-1.5.0.jar \
            -- gs://vale-dataops-bucket/data/customer_data_dirty.csv gs://vale-dataops-bucket/data/payment_data_dirty.csv


